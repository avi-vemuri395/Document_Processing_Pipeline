
## ðŸ—ï¸ Architecture Overview

### Two-Part Pipeline Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PART 1: DOCUMENT PROCESSING                    â”‚
â”‚     Documents â†’ Comprehensive Extraction â†’ Master JSON Pool      â”‚
â”‚                                                                 â”‚
â”‚  ðŸ“„ Input: PDF, Excel, Images                                   â”‚
â”‚  ðŸ”„ Process: unstructured json -> Claudeâ†’ Structured JSON       â”‚
â”‚  ðŸ’¾ Output: master_data.json (ALL extracted data)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PART 2: FORM GENERATION                       â”‚
â”‚  Master JSON â†’ Intelligent Mapping â†’ 9 Bank Forms + PDFs        â”‚
â”‚                                                                 â”‚
â”‚  ðŸŽ¯ Live Oak: Application, PFS, 4506-T                         â”‚
â”‚  ðŸ¦ Huntington: Business App, Tax Transcript, Debt Schedule    â”‚
â”‚  ðŸ§ Wells Fargo: Financial Questionnaire, Business Info        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Principle**: Extract data ONCE, map to MANY outputs

## ðŸš€ Quick Start

### Environment Setup
```bash
# Install dependencies
pip install -r requirements-minimal.txt
```

### Run Tests
```bash
# Fast test (2 PDFs + 2 Excel, ~2 minutes)
python3 run_fast_test.py

# Comprehensive test (19 documents, ~8 minutes)
python3 run_comprehensive_test.py

# Analyze PDF structure (no API calls)
python3 tests/analysis/test_pdf_technical_structure.py
```

### Basic Usage
```python
from src.template_extraction import PipelineOrchestrator

orchestrator = PipelineOrchestrator()

# Process documents incrementally (simulates prod)
results = await orchestrator.process_application(
    application_id="app_001",
    documents=[
        Path("inputs/real/Brigham_dallas/Brigham_Dallas_PFS.pdf"),
        Path("inputs/real/Brigham_dallas/Hello_Sugar_LLC_2023.pdf")
    ],
    target_banks=["live_oak", "huntington"],
    generate_spreadsheets=True
)
```

## ðŸ”§ Core Components

### Part 1: Document Extraction

#### **PipelineOrchestrator** (`pipeline_orchestrator.py`)
- **Role**: Main coordinator for the entire pipeline
- **Features**: Incremental processing, bank selection, output management
- **Input**: Document paths, application ID, target banks
- **Output**: Complete results with forms, PDFs, and spreadsheets

#### **ComprehensiveProcessor** (`comprehensive_processor.py`) 
- **Role**: Extract ALL data from documents ONCE (Part 1 implementation)
- **Method**: Uses BenchmarkExtractor â†’ merges with existing master JSON
- **Key Feature**: Deep merge logic preserves data across incremental document additions
- **Output**: `master_data.json` with comprehensive structured data

#### **BenchmarkExtractor** (`benchmark_extractor.py`)
- **Role**: Core extraction engine using Claude 4 Sonnet Vision API
- **Process**: Document â†’ UniversalPreprocessor â†’ Images â†’ Claude Vision â†’ JSON
- **Model**: `claude-sonnet-4-20250514`
- **Features**: 
  - Supports Files API mode for native PDF processing (rate limit issues)
  - Automatic image optimization (resolution, contrast)
  - Comprehensive financial data extraction with validation

#### **UniversalPreprocessor** (`universal_preprocessor.py`)
- **Role**: Convert ANY document format to optimized images for Claude Vision
- **Supported**: PDF, Excel (.xlsx/.xls), Images (PNG/JPG), Text files
- **Process**: 
  - PDF â†’ High-resolution images (pdf2image)
  - Excel â†’ HTML tables â†’ Screenshot images (pandas + matplotlib)
  - Images â†’ Resolution and contrast optimization
- **Output**: List of enhanced PIL Images ready for Vision API

### Part 2: Form Generation

#### **FormMappingService** (`form_mapping_service.py`)
- **Role**: Map master JSON to 9 different bank forms (Part 2a implementation)
- **Features**:
  - Intelligent field matching with variations (SSN = social_security_number)
  - Deep flattening to extract leaf values from nested JSON
  - Form specification loading from `templates/form_specs/`
  - Coverage calculation and validation
- **Output**: Form-specific JSON mappings + PDF generation

#### **PDFFormGenerator** (`pdf_form_generator.py`)
- **Role**: Fill actual PDF forms with extracted data
- **Technology**: PyPDFForm for deterministic field filling
- **Features**:
  - Text field mapping with data validation
  - Checkbox state management (handles various PDF checkbox formats)
  - AcroForm field discovery and mapping
- **Output**: Filled PDF forms ready for bank submission

#### **SpreadsheetMappingService** (`spreadsheet_mapping_service.py`)
- **Role**: Generate Excel spreadsheets from master JSON (Part 2b implementation)
- **Templates**: Debt Schedule, Use of Funds, Financial Projections
- **Technology**: openpyxl for Excel template population
- **Output**: Completed Excel files with extracted data

## ðŸ” Document Processing Intelligence

### Document Type Handling
- **Digital PDFs**: Excellent performance (95%+ accuracy)
- **Scanned PDFs**: Good performance (Claude Vision sort of handles scan artifacts)
- **Excel Files**: Need multi page
- **Mixed Content**: Handles varying quality and formats

### Validation
- **Calculation Validation**: Automatically verifies math (eg assets - liabilities = net worth)
- **Cross-Reference Checking**: Validates data consistency across documents
- **Missing Field Detection**: Identifies incomplete extractions

## ðŸ“ Project Structure

```
Document_Processing_Pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ template_extraction/           # Two-part pipeline
â”‚   â”‚   â”œâ”€â”€ pipeline_orchestrator.py  # Main coordinator
â”‚   â”‚   â”œâ”€â”€ comprehensive_processor.py # Part 1: Extract ONCE
â”‚   â”‚   â”œâ”€â”€ form_mapping_service.py    # Part 2a: Map to forms
â”‚   â”‚   â””â”€â”€ spreadsheet_mapping_service.py # Part 2b: Excel generation
â”‚   â””â”€â”€ extraction_methods/
â”‚       â””â”€â”€ multimodal_llm/
â”‚           â”œâ”€â”€ providers/
â”‚           â”‚   â”œâ”€â”€ benchmark_extractor.py # Core Claude Vision engine
â”‚           â”‚   â””â”€â”€ pdf_form_generator.py  # PDF filling
â”‚           â””â”€â”€ core/
â”‚               â””â”€â”€ universal_preprocessor.py # Document â†’ Image conversion
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ form_specs/                   # JSON specs for 9 bank forms
â”‚   â”œâ”€â”€ Live Oak Express - Application Forms.pdf
â”‚   â”œâ”€â”€ Huntington Bank Personal Financial Statement.pdf
â”‚   â””â”€â”€ *.xlsx                        # Excel templates
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ integration/                  # End-to-end pipeline tests
â”‚   â”œâ”€â”€ analysis/                     # PDF structure analysis
â”‚   â””â”€â”€ pipeline/                     # Component-specific tests
â”œâ”€â”€ inputs/real/                      # Test documents
â””â”€â”€ outputs/                          # Generated results
```

## ðŸ”® Future Enhancements

### Priority 2: Document Intelligence Router
**Goal**: Optimize processing based on document characteristics
**Implementation**:
```python
class DocumentRouter:
    def classify_document(self, path) -> DocumentType:
        # Digital PDF â†’ Direct Vision API
        # Scanned PDF â†’ Enhanced preprocessing  
        # Excel â†’ Direct parsing or improved imaging
        # Complex tables â†’ Specialized extraction
```

### Priority 3: Advanced Extraction Features
- **Multi-page table recognition**: Handle tables spanning multiple pages
- **Confidence scoring per field**: Quality metrics for each extracted value

### Priority 4: Production Optimizations
- **Batch processing**: Process multiple documents in parallel
- **Caching layer**: Cache extraction results for duplicate documents
- **Incremental updates**: Smart re-processing when documents change
- **API rate limiting**: Intelligent chunking

## ðŸ§ª Testing & Validation

### Test packets ###
- **Brigham Dallas Package**: 19 files (PFS, tax returns 2021-2024, business docs)
- **Dave Burlington Package**: Complete loan application with projections

## ðŸ“ž Support & Development

### Environment Requirements
- **Python**: 3.13.3+
- **API Key**: Anthropic Claude API access
- **System Dependencies**: 
  - `poppler-utils` for PDF processing
  - Standard ML libraries (PIL, pandas, etc.)

### Key Commands
```bash
# Setup
python3 check_env.py

# Quick validation  
python3 run_fast_test.py

# Full system test
python3 run_comprehensive_test.py

# Analyze document types
python3 tests/analysis/test_pdf_technical_structure.py
```

### Common Issues
1. **API Rate Limits**: Reduce document batch sizes
2. **Memory Usage**: Large documents may require chunking

---

**Architecture Version**: Two-Part Pipeline v2.0  
**Claude Model**: claude-sonnet-4-20250514